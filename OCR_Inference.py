# -*- coding: utf-8 -*-
"""
Original file is located at
    https://colab.research.google.com/drive/1iUosbPRpbgwFSMS0vR3dQes5x_wq_Ydv
"""

import warnings
warnings.filterwarnings('ignore')
from collections import defaultdict
from PIL import Image
import torch
import cv2
import os
import re
import gc
from tqdm import tqdm
import numpy as np
import pandas as pd
from transformers import AutoProcessor, VisionEncoderDecoderModel
import math
from collections import defaultdict

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

p1_processor = AutoProcessor.from_pretrained("Laskari-Naveen/pytorch_8April_Table_RC", token = "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")
p1_model = VisionEncoderDecoderModel.from_pretrained("Laskari-Naveen/pytorch_8April_Table_RC", token = "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").to(device)

# p2_processor = AutoProcessor.from_pretrained("/content/models/content/huggingface_models/models--Laskari-Naveen--UB_Table_Model/snapshots/5fc871e156f3c46ba4704d3e5bda8a080008a40c")
# p2_model = VisionEncoderDecoderModel.from_pretrained("/content/models/content/huggingface_models/models--Laskari-Naveen--UB_Table_Model/snapshots/5fc871e156f3c46ba4704d3e5bda8a080008a40c").to(device)

def free_memory():
    gc.collect()
    torch.cuda.empty_cache()

def run_prediction(image, model, processor):
    pixel_values = processor(image, return_tensors="pt").pixel_values
    task_prompt = "<s>"
    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids
    outputs = model.generate(
        pixel_values.to(device),
        decoder_input_ids=decoder_input_ids.to(device),
        max_length=model.decoder.config.max_position_embeddings,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
        use_cache=True,
        num_beams=2,
        output_scores=True,
        bad_words_ids=[[processor.tokenizer.unk_token_id]],
        return_dict_in_generate=True,
    )
    # process output
    scores = outputs.scores
    prediction = processor.batch_decode(outputs.sequences)[0]
    prediction = prediction.replace("<one>", "1")
    prediction = processor.token2json(prediction)
    return prediction, outputs, scores

def convert_string_to_dict(xml_input):
    # Use regex to extract key-value pairs
    pattern = r"<(\d+_[a-zA-Z]+)>(.*?)</\1>"
    matches = re.findall(pattern, xml_input, re.DOTALL)

    result_dict = {}
    for key, value in matches:
        # Extract individual row values using another regex
        row_pattern = r"<row\d+>(.*?)</row\d+>"
        row_values = re.findall(row_pattern, value, re.DOTALL)

        # Clean up the row values and handle potential missing values.
        cleaned_row_values = []
        for row_val in row_values:
          cleaned_row_val = row_val.strip()
          if not cleaned_row_val:
            cleaned_row_values.append("[BLANK]")
          else:
            cleaned_row_values.append(cleaned_row_val)
        result_dict[key] = cleaned_row_values
    return result_dict

def convert_string_to_dict_v2(xml_input):
    # Use regex to extract key-value pairs
    pattern = r"<(\d+_[a-zA-Z]+)>(.*?)</\1>"
    matches = re.findall(pattern, xml_input, re.DOTALL)
    result_dict = {}
    for key, value in matches:
        # Extract individual row values using another regex
        row_pattern = r"<row\d+>(.*?)</row\d+>"
        row_values = re.findall(row_pattern, value, re.DOTALL)

        # Clean up the row values and handle potential missing values
        cleaned_row_values = []
        for row_val in row_values:
            cleaned_row_val = row_val.strip()
            if not cleaned_row_val:
                cleaned_row_values.append("[BLANK]")
            else:
                cleaned_row_values.append(cleaned_row_val)

        # Format as single string with values in single quotes, separated by semicolons
        formatted_value = ";".join(f"{val}" for val in cleaned_row_values)
        result_dict[key] = formatted_value
    return result_dict

def split_and_expand(row):
    keys = [row['Key']] * len(row['Value'].split(';'))
    values = row['Value'].split(';')
    return pd.DataFrame({'Key': keys, 'Value': values})

def calculate_key_aggregated_scores(scores, outputs, processor):
    """
    Calculate aggregated scores for each key from model outputs and scores.

    Args:
        scores (list): The list of score tensors for each decoding step.
        outputs (obj): The output object from the model containing generated sequences.
        processor (obj): The processor object for decoding tokens.

    Returns:
        dict: A dictionary with keys as the tokenized keys and their aggregated scores.
    """
    key_aggregated_scores = defaultdict(float)
    # Token IDs generated by the model (excluding input tokens like <s>)
    generated_token_ids = outputs.sequences[0][1:]  # Exclude the first token (<s>)
    current_key = None  # Track the current key during decoding
    token_scores = []  # Temporary list to store scores of intermediate tokens
    row_scores = []  # Temporary list for scores within semicolon-separated rows

    for idx, (score, token_id) in enumerate(zip(scores, generated_token_ids)):
        # Decode the token
        token = processor.tokenizer.decode([token_id.item()], skip_special_tokens=False)
        # Handle token when it's <one>
        if token == "<one>":
            token = "1"
        # Detect the start of a new key
        if token.startswith("<s_") and not token.startswith("</"):
            # print(f"Start of the token {token}")
            # Start a new key; reset token scores.
            # From a text remove <s_ and >
            current_key = token[3:-1]
            # current_key = token
            token_scores = []
            row_scores = []

        # Detect the end of the current key
        elif token.startswith("</") and current_key is not None:

            # print(f"End of the token {token}")
            # Compute the aggregated score for the key
            if token_scores:
                product_of_scores = math.prod(token_scores)
                aggregated_score = product_of_scores ** (1 / len(token_scores))
                row_scores.append(aggregated_score)

            # Assign row scores to the key
            key_aggregated_scores[current_key] = row_scores if len(row_scores) > 1 else row_scores[0]
            current_key = None  # Reset the key tracking

        # Process intermediate tokens
        elif current_key is not None:
            # Calculate the token's probability
            max_score = torch.softmax(score, dim=-1).max().item()
            # Handle row separators
            if token == ";":
                # print("Calculating Intermedeate")
                # Calculate and store the score for the current row
                if token_scores:
                    product_of_scores = math.prod(token_scores)
                    aggregated_score = product_of_scores ** (1 / len(token_scores))
                    row_scores.append(aggregated_score)
                    token_scores = []  # Reset token scores for the next row
            elif not token.startswith("<") and not token.startswith("</"):
                # Include the score for intermediate tokens
                # print(processor.tokenizer.decode([token_id.item()], skip_special_tokens=False))
                # print(max_score)
                token_scores.append(max_score)
    return key_aggregated_scores

def calculate_key_aggregated_scores_tables(scores, outputs, processor):
    """
    Calculate aggregated scores for each key from model outputs and scores.
    Updated to work with the new format using <row> tags.

    Args:
        scores (list): The list of score tensors for each decoding step.
        outputs (obj): The output object from the model containing generated sequences.
        processor (obj): The processor object for decoding tokens.

    Returns:
        dict: A dictionary with keys as the field keys and their aggregated scores.
    """
    key_aggregated_scores = defaultdict(float)

    # Token IDs generated by the model (excluding input tokens like <s>)
    generated_token_ids = outputs.sequences[0][1:]  # Exclude the first token (<s>)

    current_key = None  # Track the current key during decoding
    current_row = None  # Track the current row
    token_scores = []  # Temporary list to store scores of tokens in current row
    key_row_scores = {}  # Dictionary to store scores for each row in a key

    for idx, (score, token_id) in enumerate(zip(scores, generated_token_ids)):
        # Decode the token
        token = processor.tokenizer.decode([token_id.item()], skip_special_tokens=False)

        # Handle token when it's <one>
        if token == "<one>":
            token = "1"

        # Detect start of a key (e.g., <48_NonCovChrg>)
        if re.match(r"<\d+_[a-zA-Z]+>", token):
            # Extract key name without < >
            current_key = token[1:-1]
            key_row_scores[current_key] = []

        # Detect start of a row (e.g., <row1>)
        elif re.match(r"<row\d+>", token) and current_key is not None:
            current_row = token
            token_scores = []

        # Detect end of a row (e.g., </row1>)
        elif re.match(r"</row\d+>", token) and current_key is not None and current_row is not None:
            # Calculate aggregated score for this row
            if token_scores:
                product_of_scores = math.prod(token_scores)
                aggregated_score = product_of_scores ** (1 / len(token_scores))
                key_row_scores[current_key].append(aggregated_score)
            current_row = None

        # Detect end of a key (e.g., </48_NonCovChrg>)
        elif re.match(r"</\d+_[a-zA-Z]+>", token) and current_key is not None:
            # Store all row scores for this key
            if len(key_row_scores[current_key]) > 0:
                key_aggregated_scores[current_key] = key_row_scores[current_key]
            current_key = None

        # Process tokens inside a row
        elif current_key is not None and current_row is not None:
            # Calculate the token's probability
            max_score = torch.softmax(score, dim=-1).max().item()

            # Only include normal content tokens, not formatting/special tokens
            if not token.startswith("<") and not token.startswith("</"):
                token_scores.append(max_score)

    # Convert single-item lists to scalar values for compatibility with original function
    for key, scores_list in key_aggregated_scores.items():
        if len(scores_list) == 1:
            key_aggregated_scores[key] = scores_list[0]

    return dict(key_aggregated_scores)


test_image_dir = r'/content/drive/MyDrive/001_projects/FSL/FSL_2025/Processing_stage_2(Img_Json)/UB/evaluation_dataset_sprint4/Image'
Extraction_result =  pd.DataFrame()
test_images = os.listdir(test_image_dir)
free_memory()
final_df = pd.DataFrame()
count = 0

for each_test_image in tqdm(test_images):
    print(each_test_image)
    test_image_path = '/content/UB_1.jpeg'
    # test_image_path = os.path.join(test_image_dir, each_test_image)
    try:
        expanded_df = pd.DataFrame()
        result_df_each_image = pd.DataFrame()
        image = Image.open(test_image_path).convert("RGB")
        # prediction_1, output_1, scores_1 = run_prediction(image, p1_model, p1_processor)
        prediction_2, output_2, scores_2 = run_prediction(image, p1_model, p1_processor)
        op = convert_string_to_dict(prediction_2['text_sequence'])
        op['Image_Name'] = each_test_image
        final_df = pd.concat([final_df, pd.DataFrame(op)], ignore_index=True)
    except Exception as e:
        print(e)
    count += 1
    if count == 5:
        break
    free_memory()
    break

convert_string_to_dict(prediction_2['text_sequence'])
convert_string_to_dict_v2(prediction_2['text_sequence'])
calculate_key_aggregated_scores_tables(scores_2, output_2, p1_processor)
final_df.to_excel("/content/drive/MyDrive/XELP_PROJECTS/FSL_OCR/FSL_2025/Results/UB/UB_validation_results_231_images_pytorch_8th_April_Table_col_row.xlsx",index=False)
final_df.to_parquet("/content/drive/MyDrive/XELP_PROJECTS/FSL_OCR/FSL_2025/Results/UB/UB_validation_results_231_images_pytorch_8th_April_Table_col_row.parquet",index=False)
pd.set_option('display.max_rows', 300)